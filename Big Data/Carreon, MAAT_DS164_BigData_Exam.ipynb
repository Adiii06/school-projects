{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b147b11d-72e6-4384-b58b-1e52a21fc1b8",
   "metadata": {},
   "source": [
    "## DS164 Big Data Exam:\n",
    "- Prepared by: Van Julius Leander G. Lopez, MSDS\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<b>INSTRUCTION</b>: CONCEPTUAL SECTION: Multiple Choice but you will be putting your answers in a dictionary as shown below (This is for easier checking). \n",
    "</div\n",
    "\n",
    "```python\n",
    "{1: 'a',\n",
    " 2: 'c',\n",
    " 3: 'b',\n",
    " 4: 'd',\n",
    " 5: 'd'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c753cda-d55a-4db8-8dcf-5ee967fff258",
   "metadata": {},
   "source": [
    "#### PySpark\n",
    "\n",
    "1. What is PySpark?\n",
    "\n",
    "- A) A Python wrapper for Apache Spark.\n",
    "- B) A standalone data processing framework.\n",
    "- C) A SQL query engine.\n",
    "- D) A machine learning library for Python.\n",
    "\n",
    "2. Which method is used to read a CSV file into a Spark DataFrame?\n",
    "\n",
    "- A) read.csv()\n",
    "- B) pd.read_csv()\n",
    "- C) spark.read.csv()\n",
    "- D) df.read.csv()\n",
    "\n",
    "3. How do you remove duplicate rows in a Spark DataFrame based on specific columns?\n",
    "\n",
    "- A) df.dropDuplicates()\n",
    "- B) df.drop_duplicates()\n",
    "- C) df.distinct()\n",
    "- D) df.drop_duplicates([\"column_name\"])\n",
    "\n",
    "4. Which function is used to group data in a PySpark DataFrame?\n",
    "\n",
    "- A) df.groupBy()\n",
    "- B) df.groupby()\n",
    "- C) df.agg()\n",
    "- D) df.group()\n",
    "\n",
    "5. Which of the following is not a method to filter rows in a PySpark DataFrame?\n",
    "\n",
    "- A) filter()\n",
    "- B) where()\n",
    "- C) select()\n",
    "- D) limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30616d5a-4e8c-49be-8b3f-e484af093ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Dictionary Here\n",
    "pyspark_dict = {\n",
    "    1: 'a',\n",
    "    2: 'c',\n",
    "    3: 'd',\n",
    "    4: 'a',\n",
    "    5: 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e47d3c-7d1f-4ba2-b177-bceaba21fa83",
   "metadata": {},
   "source": [
    "#### PySQL\n",
    "\n",
    "1. What is PySQL primarily used for?\n",
    "\n",
    "- A) Interacting with SQL databases from Python.\n",
    "- B) Performing machine learning tasks.\n",
    "- C) Creating Spark DataFrames.\n",
    "- D) Reading and writing CSV files.\n",
    "\n",
    "2. Which Python library is commonly used for working with SQL databases?\n",
    "\n",
    "- A) pandas\n",
    "- B) numpy\n",
    "- C) sqlalchemy\n",
    "- D) matplotlib\n",
    "\n",
    "3. How do you create a SQLAlchemy engine for a SQLite database?\n",
    "\n",
    "- A) create_engine('sqlite:///database.db')\n",
    "- B) sqlite3.connect('database.db')\n",
    "- C) pd.read_sql('database.db')\n",
    "- D) sql.connect('database.db')\n",
    "\n",
    "4. Which method is used to execute a raw SQL query in SQLAlchemy?\n",
    "\n",
    "- A) execute()\n",
    "- B) run()\n",
    "- C) query()\n",
    "- D) commit()\n",
    "\n",
    "5. What is the primary purpose of an ORM (Object-Relational Mapper)?\n",
    "\n",
    "- A) To directly execute SQL commands.\n",
    "- B) To map Python objects to database tables.\n",
    "- C) To read CSV files into DataFrames.\n",
    "- D) To perform machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e263cb-844b-4080-bcb7-2710a6c2ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Dictionary Here\n",
    "pysql_dict = {\n",
    "    1: 'a',\n",
    "    2: 'c',\n",
    "    3: 'a',\n",
    "    4: 'a',\n",
    "    5: 'b'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca1705f-3598-4069-992b-49a69ecf1192",
   "metadata": {},
   "source": [
    "#### Spark ML\n",
    "\n",
    "1. What is Spark MLlib?\n",
    "\n",
    "- A) A distributed machine learning library.\n",
    "- B) A data visualization tool.\n",
    "- C) A SQL query engine.\n",
    "- D) A Python IDE.\n",
    "\n",
    "2. Which method is used to split data into training and test sets in Spark MLlib?\n",
    "\n",
    "- A) split()\n",
    "- B) randomSplit()\n",
    "- C) train_test_split()\n",
    "- D) partition()\n",
    "\n",
    "3. Which Spark MLlib component is used for building machine learning pipelines?\n",
    "\n",
    "- A) Pipeline\n",
    "- B) Workflow\n",
    "- C) Sequence\n",
    "- D) Process\n",
    "\n",
    "4. How do you specify the features and label in a Spark MLlib VectorAssembler?\n",
    "\n",
    "- A) VectorAssembler(inputCols=[\"features\"], outputCol=\"label\")\n",
    "- B) VectorAssembler(inputCols=[\"label\"], outputCol=\"features\")\n",
    "- C) VectorAssembler(inputCols=[\"features\"], labelCol=\"label\")\n",
    "- D) VectorAssembler(labelCols=[\"label\"], outputCol=\"features\")\n",
    "\n",
    "5. Which algorithm is used for classification tasks in Spark MLlib?\n",
    "\n",
    "- A) LogisticRegression\n",
    "- B) LinearRegression\n",
    "- C) KMeans\n",
    "- D) ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8326a7e-071c-4310-b82b-92c1580a1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Dictionary Here\n",
    "sparkml_dict = {\n",
    "    1: 'a',\n",
    "    2: 'b',\n",
    "    3: 'a',\n",
    "    4: 'b',\n",
    "    5: 'a'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774c9d4-2c3a-4895-aeda-f40b24c90de7",
   "metadata": {},
   "source": [
    "The cell below is what I will be using to check your multiple choice section. Please leave it commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31254f17-7d32-454e-bb96-8b476d5f89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def load_json(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# correct_pyspark = load_json('answer_keys/PySpark.json')\n",
    "# correct_pysql = load_json('answer_keys/PySQL.json')\n",
    "# correct_sparkml = load_json('answer_keys/SparkML.json')\n",
    "\n",
    "# def check_answers(correct_answers, student_answers):\n",
    "#     correct_count = 0\n",
    "#     wrong_count = 0\n",
    "    \n",
    "#     for q, correct_ans in correct_answers.items():\n",
    "#         student_ans = student_answers.get(int(q))\n",
    "#         if student_ans == correct_ans:\n",
    "#             correct_count += 1\n",
    "#         else:\n",
    "#             wrong_count +=1\n",
    "    \n",
    "#     return correct_count, wrong_count\n",
    "\n",
    "# correct_count_pyspark, wrong_items_pyspark = check_answers(correct_pyspark, pyspark_dict)\n",
    "# correct_count_pysql, wrong_items_pysql = check_answers(correct_pysql, pysql_dict)\n",
    "# correct_count_sparkml, wrong_items_sparkml = check_answers(correct_sparkml, sparkml_dict)\n",
    "\n",
    "# print(f'PySpark - Correct: {correct_count_pyspark}, Wrong: {wrong_items_pyspark}')\n",
    "# print(f'PySQL - Correct: {correct_count_pysql}, Wrong: {wrong_items_pysql}')\n",
    "# print(f'SparkML - Correct: {correct_count_sparkml}, Wrong: {wrong_items_sparkml}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332e98b-38de-4fc6-8148-b7a853e6c53e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<b>INSTRUCTION</b>: CODING SECTION: In this examination, we are going to be using a dataset from kaggle named 114000 Spotify Songs. The link to the dataset will be provided below the instruction section. There will be coding tasks and both visible and invisible asserts. The invisible asserts will be used as another layer of checking (To see if you get max credits). When creating functions, please include a doc string in numpy format. Best of luck!\n",
    "</div\n",
    "\n",
    "\n",
    "Clink the [LINK](https://www.kaggle.com/datasets/priyamchoksi/spotify-dataset-114k-songs) for more details about the dataset. (I will be providing the same dataset as an attachment for your exam but just in case you are not able to access it, you can download it from the link provided)\n",
    "\n",
    "Here is the data dictionary for our dataset:\n",
    "\n",
    "| Column           | Description                                                                                          |\n",
    "|------------------|------------------------------------------------------------------------------------------------------|\n",
    "| track_id         | The unique Spotify ID for each track.                                                                |\n",
    "| artists          | Names of the artists who performed the track, separated by ';'.                                      |\n",
    "| album_name       | The name of the album in which the track appears.                                                    |\n",
    "| track_name       | The title of the track.                                                                              |\n",
    "| popularity       | A value between 0 and 100, indicating the track's popularity based on recent plays.                  |\n",
    "| duration_ms      | The length of the track in milliseconds.                                                             |\n",
    "| explicit         | Boolean indicating whether the track contains explicit content.                                      |\n",
    "| danceability     | Describes how suitable a track is for dancing (0.0 = least danceable, 1.0 = most danceable).         |\n",
    "| energy           | Represents the intensity and activity of a track (0.0 = low energy, 1.0 = high energy).              |\n",
    "| key              | The musical key of the track mapped using standard Pitch Class notation.                             |\n",
    "| loudness         | Overall loudness of the track in decibels (dB).                                                      |\n",
    "| mode             | Indicates the modality (major or minor) of the track.                                                |\n",
    "| speechiness      | Detects the presence of spoken words in the track.                                                   |\n",
    "| acousticness     | Confidence measure of whether the track is acoustic (0.0 = not acoustic, 1.0 = highly acoustic).     |\n",
    "| instrumentalness | Predicts whether a track contains vocals (0.0 = contains vocals, 1.0 = instrumental).                |\n",
    "| liveness         | Detects the presence of an audience in the recording (0.0 = studio recording, 1.0 = live performance).|\n",
    "| valence          | Measures the musical positiveness conveyed by a track (0.0 = negative, 1.0 = positive).              |\n",
    "| tempo            | Estimated tempo of the track in beats per minute (BPM).                                              |\n",
    "| time_signature   | Estimated time signature of the track (3 to 7).                                                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17ecad9-4df2-4a3b-853d-85e8bd0c7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from numpy.testing import assert_array_equal, assert_equal\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .master('local[*]')\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661dcb1-8997-48e0-a811-a5aa8693bfdc",
   "metadata": {},
   "source": [
    "## Coding Task 1:\n",
    "***\n",
    "\n",
    "Create a function `read_data` that takes in the path to the dataset provided, reads the data, removes nulls with respect to the track_name column, drops duplicates with respect to the track_id column, and arranges the dataframe by track_id in ascending order. #TIP: Read it as a pandas df first then convert to a spark df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42383e4c-1c27-4976-9306-22e58a6ad625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used chatGPT, here is my prompt: create a sample of a doc string in numpy format \"my code\"\n",
    "# Note: I just used the sample as my guide, but didn't copy the whole text\n",
    "# Insert Your Code Here\n",
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads dataset as pandas df from file path, then converts it to spark df. \n",
    "    After that, performs dropping of rows with missing track names, dropping duplicates \n",
    "    based on track id, and arranging track id in ascending order.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        A cleaned Spark DF and in ascending order by track_id.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    sdf = spark.createDataFrame(df)\n",
    "    sdf = (\n",
    "        sdf.dropna(subset=['track_name'])\n",
    "        .dropDuplicates(subset=['track_id'])\n",
    "        .orderBy(col('track_id').asc())\n",
    "    )\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed74479-d9c8-4d24-96c9-8fb2a48ac505",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"dataset.csv\"\n",
    "sorted_df = read_data(file_path)\n",
    "sdf = sorted_df.toPandas()\n",
    "\n",
    "assert_equal(sdf['track_name'][0],'Lolly')\n",
    "assert_equal(sdf.shape, (89741, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d660f6b-036f-48de-9fb5-c281729a48da",
   "metadata": {},
   "source": [
    "## Coding Task 2:\n",
    "***\n",
    "\n",
    "Create a function `data_and_types` that takes in the dataframe and returns a tuple of the column names and data types of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3864a01d-c893-412e-8e2d-b64bb42fe5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def data_and_types(df):\n",
    "    \"\"\"\n",
    "    Returns the column names and data types of the df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - list of column name\n",
    "        - list of data types of the columns\n",
    "    \"\"\"\n",
    "    cnames = df.columns\n",
    "    ctypes = [dtype for _, dtype in df.dtypes]\n",
    "    \n",
    "    return cnames, ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22934cd4-c4d7-40ce-b952-cdbfb3231b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames, ctypes = data_and_types(sorted_df)\n",
    "assert_equal(len(cnames), 20)\n",
    "assert_equal(ctypes[4], 'bigint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61475f88-75bc-47f3-8d10-912b254a801c",
   "metadata": {},
   "source": [
    "## Coding Task 3:\n",
    "***\n",
    "\n",
    "Create a function `artists` that takes in the dataframe and returns the unique artists (or groups) in the dataset arranged in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebca4ef1-3ccf-4b4e-9027-8626a14b4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def artists(df):\n",
    "    \"\"\"\n",
    "    Returns the unique artists in ascending order.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        A spark df of unique artists sorted ascending order.\n",
    "    \"\"\"\n",
    "    artists_df = df.select('artists').distinct().orderBy('artists')\n",
    "    \n",
    "    return artists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdf2461-2243-4d71-931d-364c3c73064d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist_df = artists(sorted_df).toPandas()\n",
    "assert_equal(artist_df['artists'].iloc[4322], 'Bury Your Dead')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e37474-0d5b-4e11-95bf-08541c4e4673",
   "metadata": {},
   "source": [
    "## Coding Task 4:\n",
    "***\n",
    "\n",
    "Create a function `ave_dur` that takes in the dataframe and returns the average duration of all the trachs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a7e670-89d7-4b26-8e8c-463f84a031a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def ave_dur(df):\n",
    "    \"\"\"\n",
    "    Returns the average duration of all tracks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Average duration of tracks in ms.\n",
    "    \"\"\"\n",
    "    avg_d = df.agg({'duration_ms': 'avg'}).collect()[0][0]\n",
    "    \n",
    "    return avg_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0ed501-1454-4bf9-92be-ee09d8fc4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(ave_dur(sorted_df), 229141.81218172295)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3524d0-243c-44bb-ad65-5e905b1fd34d",
   "metadata": {},
   "source": [
    "## Coding Task 5:\n",
    "***\n",
    "\n",
    "Create a function `explicit_tracks` that takes in the dataframe and returns the number of tracks marked as explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17bdd0d2-dc6f-4fd7-a077-a68ab7c5976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def explicit_tracks(df):\n",
    "    \"\"\"\n",
    "    Returns the number of explicit tracks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of explicit tracks.\n",
    "    \"\"\"\n",
    "    explicit_num = df.filter(col('explicit') == True).count()\n",
    "    \n",
    "    return explicit_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ec8ac6-45d9-4ad2-bbef-e4de91bd0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(explicit_tracks(sorted_df), 7704)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f840bb3-b26e-4687-9ab2-9ec0e90200bf",
   "metadata": {},
   "source": [
    "## Coding Task 6:\n",
    "***\n",
    "Create a function `pop_track` that takes in the dataframe and returns the most popular track and its popularity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a76ceb5-01a0-4270-959b-7db3825ff604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def pop_track(df):\n",
    "    \"\"\"\n",
    "    Returns the most popular track and its popularity score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - most popular track name\n",
    "        - popularity score of the track\n",
    "    \"\"\"\n",
    "    most_pop_track = df.orderBy(col('popularity').desc()).first()\n",
    "    track = most_pop_track['track_name']\n",
    "    pop = most_pop_track['popularity']\n",
    "    \n",
    "    return track, pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ed05f52-c999-4499-bcbb-a4f5186e761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "track, pop = pop_track(sorted_df)\n",
    "assert_equal(track, 'Unholy (feat. Kim Petras)')\n",
    "assert_equal(pop, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d21f0-b626-49b7-b7a8-c8c35674e07e",
   "metadata": {},
   "source": [
    "## Coding Task 7:\n",
    "***\n",
    "\n",
    "Create a function `major_minor` that takes in the dataframe and returns the number of tracks in major mode (1) and in minor mode (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d3000fb-567f-4eca-be80-887c5431ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def major_minor(df):\n",
    "    \"\"\"\n",
    "    Returns the number of tracks in major (1) and minor mode (0).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - number of tracks in major mode\n",
    "        - number of tracks in minor mode\n",
    "    \"\"\"\n",
    "    major_mode = df.filter(col('mode') == 1).count()\n",
    "    minor_mode = df.filter(col('mode') == 0).count()\n",
    "    \n",
    "    return major_mode, minor_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f992b279-ab80-4580-b65e-44c8a7df593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_mode, minor_mode = major_minor(sorted_df)\n",
    "assert_equal(major_mode, 57162)\n",
    "assert_equal(minor_mode, 32579)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9dcb90-6635-4e13-89df-28d99de393a9",
   "metadata": {},
   "source": [
    "## Coding Task 8:\n",
    "***\n",
    "\n",
    "Create a function `tswizzle` that takes in the dataframe and returns the most popular track and its popularity score of Taylor Swift as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f525a9-b0d0-478e-8962-de1580b94fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def tswizzle(df):\n",
    "    \"\"\"\n",
    "    Returns the most popular track and its popularity score of Taylor Swift\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - Taylor Swift's most popular track name \n",
    "        - popularity score of the track\n",
    "    \"\"\"\n",
    "    swifties = df.filter(col('artists').contains('Taylor Swift'))\n",
    "    swifties_no1 = swifties.orderBy(col('popularity').desc()).first()\n",
    "    tname = swifties_no1['track_name']\n",
    "    tpop = swifties_no1['popularity']\n",
    "    \n",
    "    return tname, tpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4fb6526-d1f5-4dde-9f7f-b51b8929a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tname, tpop = tswizzle(sorted_df)\n",
    "assert_equal(tname, 'Don’t Blame Me')\n",
    "assert_equal(tpop, 88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e371ba7-1922-446a-b828-93d44c410fb8",
   "metadata": {},
   "source": [
    "## Coding Task 9:\n",
    "***\n",
    "Create a function `tswizzle_and_friends` that takes in the dataframe and returns the most popular song where Taylor Swift is featured with other artists, along with the names of the co-artists in that song as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fa902b-eb97-4255-85b3-4645173e3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def tswizzle_and_friends(df):\n",
    "    \"\"\"\n",
    "    Returns the most popular song of Taylor with other artists, \n",
    "    along with the names of the co-artists in that song.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - most popular song of Taylor with other artists \n",
    "        - names of the co-artists\n",
    "    \"\"\"\n",
    "    co_swifties = df.filter(col('artists')\n",
    "                            .contains('Taylor Swift') & ~col('artists')\n",
    "                             .startswith('Taylor Swift'))\n",
    "    no1 = co_swifties.orderBy(col('popularity').desc()).first()\n",
    "    tsong_new = no1['track_name']\n",
    "    tnames = no1['artists']\n",
    "    \n",
    "    return tsong_new, tnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ac6d0f-9497-4818-b486-f1ab7a2b2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsong_new, tnames = tswizzle_and_friends(sorted_df)\n",
    "assert_equal(tsong_new, 'I Don’t Wanna Live Forever (Fifty Shades Darker)')\n",
    "assert_equal(tnames, 'ZAYN;Taylor Swift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0909480-d59c-44a5-909c-69e2cbe597e6",
   "metadata": {},
   "source": [
    "## Coding Task 10:\n",
    "***\n",
    "\n",
    "Create a function `get_most_popular_album_stats` that takes a Spark DataFrame as input and returns the total duration of songs in milliseconds, average danceability, and average energy of the most popular album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b903eef-2c61-4b27-9608-02007dfea9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Your Code Here\n",
    "def get_most_popular_album_stats(df):\n",
    "    \"\"\"\n",
    "    Returns the total duration of songs in ms, average danceability, and average energy of the\n",
    "    most popular album.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The Spark DF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        containing:\n",
    "        - most popular album's total duration in ms\n",
    "        - most popular album's average danceability\n",
    "        - most popular album's average energy\n",
    "    \"\"\"\n",
    "    mpa = df.orderBy(col('popularity').desc()).first()\n",
    "    album = df.filter(col('album_name') == mpa['album_name'])\n",
    "    \n",
    "    dur = album.agg({'duration_ms': 'sum'}).collect()[0][0]\n",
    "    dance = album.agg({'danceability': 'avg'}).collect()[0][0]\n",
    "    energy = album.agg({'energy': 'avg'}).collect()[0][0]\n",
    "    \n",
    "    return dur, dance, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d46109e-b39e-47f8-ad9b-74e0cb802b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur, dance, energy = get_most_popular_album_stats(sorted_df)\n",
    "assert_equal(dur, 156943)\n",
    "assert_equal(dance, 0.714)\n",
    "assert_equal(energy, 0.472)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc22c2",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
